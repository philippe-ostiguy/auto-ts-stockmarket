common :
  start_date : '2005-07-01'
  end_date : '2024-05-23'
  max_missing_data : .02
  fetching : False
  preprocessing : False
  engineering : False
  model_phase : 'train'
  min_nb_trades : 60
  train_test_split : [.9,.10]
  val_proportion_size : .15
  min_forecasts : 50
  attributes_to_discard: ["volume","split coefficient","adjusted close", "adj close"]
  features_engineering :
    check_bell_shape: False
    pca_variance : .9
    is_using_pca : False

  hyperparameters_optimization :
    is_pruning : True
    is_optimizing : False
    nb_trials : 30
    is_using_prev_study : False

  scaling: False

inputs:
  past_covariates :
  #  - source: 'reddit'
  #    data:
  #      - 'reddit'
  #    size: 100
    - source: 'fred'
      data:
      # - 'set_dynamically'
        - 'T5YIE'
        - 'T10YIE'
        - 'T10Y3M'
        - 'DGS10'
        - 'DGS2'
        - 'DTB3'
        - 'DEXUSNZ'
        - 'VIXCLS'
        - 'T10Y2Y'
        - 'NASDAQCOM'
        - 'DCOILWTICO'

    - source: 'yahoo'
      data:
        - "GC=F"
        -  'MSFT'
        - 'GOOGL'
        - 'AAPL'
        - 'AMZN'
#          - 'set_dynamically_50_largest_stocks'
#        - 'WTI'
#        - 'NATURAL_GAS'
#        - '3month'
#        - '10year'
#        - '2year'


  future_covariates :
      data:
        - 'day'
        - 'month'

output :
  - source : 'yahoo'
    data :
      - 'SPY'

hyperparameters:
  common:
    h: 1
    input_size: 64
    max_steps: 500
    val_check_steps: 1
    batch_size: 32
    inference_windows_batch_size: -1
    valid_batch_size: 5000
    learning_rate: 0.0005
    scaler_type : 'robust'
    #optimizer : "Ranger"
    num_workers_loader: 2
    random_seed: 42
    gradient_clip_val: 1
    #early_stop_patience_steps :
    confidence_level: .6
    likelihood: [ 0.05,.4, 0.5,.6, 0.95 ]
    callbacks :
      EarlyStopping :
        monitor : 'valid_loss'
        patience : 20
        verbose : True
        mode : 'min'
      ModelCheckPoint :
        monitor : 'valid_loss'
        mode : 'min'
        #filename : 'best_model'
        save_top_k : 1
        verbose : True
    trainer_kwargs :
      callbacks:
        - 'EarlyStopping'
        - 'ModelCheckPoint'
  models :
    TFT:
      loss: 'HuberMQLoss'
      hidden_size: 256
      n_head: 8
      dropout: 0.1
      attn_dropout : 0.05
      #reduce_on_plateau_patience: 5
      #weight_decay : 0.02


hyperparameters_optimization:
  common:
    confidence_level : [.5,.6,.7,.8,.9]
    epochs: 60
    gradient_clip_val : [.1,.5,1,5]
    likelihood: [0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95]
    input_size: [16,32,64,128]
    h: 1
    batch_size: [8,16,32,64,128]
    random_seed: 42
    callbacks :
      EarlyStopping :
        monitor : 'val_PortfolioReturnMetric'
        patience : 20
        verbose : False
        mode : 'max'
      ModelCheckPoint :
        monitor : 'val_PortfolioReturnMetric'
        mode : 'max'
        save_top_k : 1
        filename : 'best_model'
        verbose : True
    trainer_kwargs :
      callbacks:
        - 'EarlyStopping'
        - 'ModelCheckPoint'

  models:
    TemporalFusionTransformer:
      loss: ['QuantileLoss','RMSE']
      learning_rate: [0.001,.1]
      hidden_size: [32,64,128,256,512]
      lstm_layers: [1,2,4]
      attention_head_size: [1,2,4,8,16]
      dropout: [0.05,0.3]
      hidden_continuous_size: [32,64,128,256,512]
      weight_decay : [0.0,0.03]
